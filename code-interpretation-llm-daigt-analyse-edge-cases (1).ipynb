{"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"sourceId":61542,"databundleVersionId":7516023,"sourceType":"competition"},{"sourceId":6977472,"sourceType":"datasetVersion","datasetId":4005256},{"sourceId":7110479,"sourceType":"datasetVersion","datasetId":4099711},{"sourceId":7117339,"sourceType":"datasetVersion","datasetId":4104703},{"sourceId":155681847,"sourceType":"kernelVersion"}],"dockerImageVersionId":30587,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"papermill":{"default_parameters":{},"duration":313.817247,"end_time":"2024-01-02T03:23:55.733941","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-01-02T03:18:41.916694","version":"2.4.0"},"widgets":{"application/vnd.jupyter.widget-state+json":{"state":{"05096c76a0334c5eb0e7170ac629b7c1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"051c486853d64c258a66a1b49bcc2e33":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_a4d57b990d1f45d59ac2958700ce1c41","max":44868,"min":0,"orientation":"horizontal","style":"IPY_MODEL_8c99a91e0e3b4951be2531ddd86f7bc2","value":44868}},"3347b5e6a5a44404ab80099727fa875b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"386db860bbb5433e85a93ed176c47b1d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"3b6eafaabcec4fe2a6d52b8eaac78026":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"75960946fe404420bd71a5d9ba910912":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3347b5e6a5a44404ab80099727fa875b","placeholder":"​","style":"IPY_MODEL_3b6eafaabcec4fe2a6d52b8eaac78026","value":"100%"}},"7c6f65d382e54031a11f02292c98f12b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_adb19f8344fc415ba01893f40d845ec1","max":3,"min":0,"orientation":"horizontal","style":"IPY_MODEL_386db860bbb5433e85a93ed176c47b1d","value":3}},"813c0f11b5954aeeae5c3cfac89ab2e4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e774ad5d4f324cbb86c7b53f0ccdcce4","placeholder":"​","style":"IPY_MODEL_9554a8042d7644c8ac088f504865bdba","value":" 3/3 [00:00&lt;00:00, 252.44it/s]"}},"8c99a91e0e3b4951be2531ddd86f7bc2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"8dc642f6987e4a52bad063d5f844081d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9554a8042d7644c8ac088f504865bdba":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"95e5193b04a34fda9e145143d819d450":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a4d57b990d1f45d59ac2958700ce1c41":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a7b54e9b3cab43359788c6ea52420ad0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_95e5193b04a34fda9e145143d819d450","placeholder":"​","style":"IPY_MODEL_eef0bce2074d43bc8d06c16d85279600","value":" 44868/44868 [01:43&lt;00:00, 409.90it/s]"}},"adb19f8344fc415ba01893f40d845ec1":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ae214558ee5c4112b8692eb51e8c470c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_75960946fe404420bd71a5d9ba910912","IPY_MODEL_7c6f65d382e54031a11f02292c98f12b","IPY_MODEL_813c0f11b5954aeeae5c3cfac89ab2e4"],"layout":"IPY_MODEL_d503bfe58d324d919f97de4ca163a15b"}},"d503bfe58d324d919f97de4ca163a15b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d92426b50ac34d4ca3850009e7d620a1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8dc642f6987e4a52bad063d5f844081d","placeholder":"​","style":"IPY_MODEL_05096c76a0334c5eb0e7170ac629b7c1","value":"100%"}},"e4e96dac651247fb89697408dc3f6875":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e774ad5d4f324cbb86c7b53f0ccdcce4":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"eef0bce2074d43bc8d06c16d85279600":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fd2cabbd5e004f5fbc56dab5001b3f2b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d92426b50ac34d4ca3850009e7d620a1","IPY_MODEL_051c486853d64c258a66a1b49bcc2e33","IPY_MODEL_a7b54e9b3cab43359788c6ea52420ad0"],"layout":"IPY_MODEL_e4e96dac651247fb89697408dc3f6875"}}},"version_major":2,"version_minor":0}}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport json\n","metadata":{"papermill":{"duration":0.671301,"end_time":"2024-01-02T03:18:45.110418","exception":false,"start_time":"2024-01-02T03:18:44.439117","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-01-24T21:00:09.627984Z","iopub.execute_input":"2024-01-24T21:00:09.628500Z","iopub.status.idle":"2024-01-24T21:00:09.960293Z","shell.execute_reply.started":"2024-01-24T21:00:09.628466Z","shell.execute_reply":"2024-01-24T21:00:09.959309Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import sys\nimport gc\n\nimport pandas as pd\nfrom sklearn.model_selection import StratifiedKFold\nimport numpy as np\nfrom sklearn.metrics import roc_auc_score\nimport numpy as np\nfrom lightgbm import LGBMClassifier\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nfrom tokenizers import (\n    decoders,\n    models,\n    normalizers,\n    pre_tokenizers,\n    processors,\n    trainers,\n    Tokenizer,\n)\n\nfrom datasets import Dataset\nfrom tqdm.auto import tqdm\nfrom transformers import PreTrainedTokenizerFast\n\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.ensemble import VotingClassifier","metadata":{"papermill":{"duration":6.501906,"end_time":"2024-01-02T03:18:51.798467","exception":false,"start_time":"2024-01-02T03:18:45.296561","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-01-24T21:00:09.962136Z","iopub.execute_input":"2024-01-24T21:00:09.962588Z","iopub.status.idle":"2024-01-24T21:00:17.937142Z","shell.execute_reply.started":"2024-01-24T21:00:09.962561Z","shell.execute_reply":"2024-01-24T21:00:17.935361Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test = pd.read_csv('/kaggle/input/llm-detect-ai-generated-text/test_essays.csv')\nsub = pd.read_csv('/kaggle/input/llm-detect-ai-generated-text/sample_submission.csv')\n# org_train = pd.read_csv('/kaggle/input/llm-detect-ai-generated-text/train_essays.csv')\n\ntrain = pd.read_csv(\"/kaggle/input/daigt-v2-train-dataset/train_v2_drcat_02.csv\", sep=',')\n","metadata":{"papermill":{"duration":1.960756,"end_time":"2024-01-02T03:18:53.78318","exception":false,"start_time":"2024-01-02T03:18:51.822424","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-01-24T21:00:17.938825Z","iopub.execute_input":"2024-01-24T21:00:17.939601Z","iopub.status.idle":"2024-01-24T21:00:20.226755Z","shell.execute_reply.started":"2024-01-24T21:00:17.939569Z","shell.execute_reply":"2024-01-24T21:00:20.225531Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We have 3 data in this competition. Since the train data given in the competition was insufficient, we used data collected by other users.","metadata":{}},{"cell_type":"code","source":"train = train.drop_duplicates(subset=['text'])\ntrain.reset_index(drop=True, inplace=True)","metadata":{"papermill":{"duration":0.061967,"end_time":"2024-01-02T03:18:53.853885","exception":false,"start_time":"2024-01-02T03:18:53.791918","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-01-24T21:00:20.228120Z","iopub.execute_input":"2024-01-24T21:00:20.229923Z","iopub.status.idle":"2024-01-24T21:00:20.297947Z","shell.execute_reply.started":"2024-01-24T21:00:20.229859Z","shell.execute_reply":"2024-01-24T21:00:20.296689Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"color: black;\n            background-color: #800000; /* Doğru bordo tonu */\n            border-radius: 10px; /* Köşeleri yuvarlamak için */\n            padding: 20px; /* Kenar boşluğu eklemek için */\n            box-shadow: 0 0 10px rgba(0, 0, 0, 0.3); /* Hafif gölgelendirme eklemek için */\">\n    <p style=\"color: white; /* Beyaz yazı için */\n              font-family: Verdana, sans-serif;\n              letter-spacing: 0.5px;\n              text-align: center; /* Metni ortalamak için */\n              margin: 0;\">\nKod Blok No:1 / Code Block No.1    </p>\n</div>\n","metadata":{}},{"cell_type":"code","source":"LOWERCASE = False\nVOCAB_SIZE = 14000000","metadata":{"papermill":{"duration":0.014485,"end_time":"2024-01-02T03:18:53.876888","exception":false,"start_time":"2024-01-02T03:18:53.862403","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-01-24T21:00:20.300185Z","iopub.execute_input":"2024-01-24T21:00:20.300709Z","iopub.status.idle":"2024-01-24T21:00:20.304409Z","shell.execute_reply.started":"2024-01-24T21:00:20.300679Z","shell.execute_reply":"2024-01-24T21:00:20.303719Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"These two variables are configuration parameters, possibly specified for use in a natural language processing (NLP) or text mining application.\n\n* LOWERCASE = False: This variable usually determines whether text data should be converted to lower case. If LOWERCASE is True, the text data will be converted to lower case; if False, it will not be converted and the original case structure will be preserved.\n\n* VOCAB_SIZE = 14000000: This variable determines the number of words to be used when possibly building a vocabulary. Especially in NLP applications, using a large vocabulary can enable the model to learn a larger vocabulary, but may require more memory and processing power. The VOCAB_SIZE parameter is usually set to control the number of words to use in the model's training process.","metadata":{}},{"cell_type":"markdown","source":"<div style=\"color: black;\n            background-color: #800000; /* Doğru bordo tonu */\n            border-radius: 10px; /* Köşeleri yuvarlamak için */\n            padding: 20px; /* Kenar boşluğu eklemek için */\n            box-shadow: 0 0 10px rgba(0, 0, 0, 0.3); /* Hafif gölgelendirme eklemek için */\">\n    <p style=\"color: white; /* Beyaz yazı için */\n              font-family: Verdana, sans-serif;\n              letter-spacing: 0.5px;\n              text-align: center; /* Metni ortalamak için */\n              margin: 0;\">\nKod Blok No:2 / Code Block No.2     </p>\n</div>\n","metadata":{}},{"cell_type":"code","source":"# Creating Byte-Pair Encoding tokenizer\nraw_tokenizer = Tokenizer(models.BPE(unk_token=\"[UNK]\"))\n\n# Adding normalization and pre_tokenizer\nraw_tokenizer.normalizer = normalizers.Sequence([normalizers.NFC()] + [normalizers.Lowercase()] if LOWERCASE else [])\nraw_tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel()\n\n# Adding special tokens and creating trainer instance\nspecial_tokens = [\"[UNK]\", \"[PAD]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]\ntrainer = trainers.BpeTrainer(vocab_size=VOCAB_SIZE, special_tokens=special_tokens)\n\n# Creating huggingface dataset object\ndataset = Dataset.from_pandas(train[['text']])\ndef train_corp_iter(): \n    for i in range(0, len(dataset), 1000):\n        yield dataset[i : i + 1000][\"text\"]\nraw_tokenizer.train_from_iterator(train_corp_iter(), trainer=trainer)\ntokenizer = PreTrainedTokenizerFast(\n    tokenizer_object=raw_tokenizer,\n    unk_token=\"[UNK]\",\n    pad_token=\"[PAD]\",\n    cls_token=\"[CLS]\",\n    sep_token=\"[SEP]\",\n    mask_token=\"[MASK]\",\n)\ntokenized_texts_test = []\n\nfor text in tqdm(test['text'].tolist()):\n    tokenized_texts_test.append(tokenizer.tokenize(text))\n\ntokenized_texts_train = []\n\nfor text in tqdm(train['text'].tolist()):\n    tokenized_texts_train.append(tokenizer.tokenize(text))","metadata":{"papermill":{"duration":103.574548,"end_time":"2024-01-02T03:20:37.459489","exception":false,"start_time":"2024-01-02T03:18:53.884941","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-01-24T21:00:20.305289Z","iopub.execute_input":"2024-01-24T21:00:20.305708Z","iopub.status.idle":"2024-01-24T21:01:53.236283Z","shell.execute_reply.started":"2024-01-24T21:00:20.305685Z","shell.execute_reply":"2024-01-24T21:01:53.235236Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":" This code seems to be designed to train a Byte-Pair Encoding (BPE) based tokenizer using Hugging Face's tokenizers and datasets libraries.\n\n* raw_tokenizer = Tokenizer(models.BPE(unk_token=\"[UNK]\")): Creating a BPE based tokenizer. The special unknown token [UNK] is defined in this tokenizer.\n\n* raw_tokenizer.normalizer and raw_tokenizer.pre_tokenizer: Normalization and pre-processing steps are added to the tokenizer. The normalization steps include Unicode normalization and optional lowercase conversion. The lowercase conversion decision is made by checking the LOWERCASE variable.\n\n* trainer = trainers.BpeTrainer(vocab_size=VOCAB_SIZE, special_tokens=special_tokens): Trainer contains the settings for training the tokenizer. vocab_size contains the size of a given vocabulary and special_tokens contains the special tokens.\n\n* dataset = Dataset.from_pandas(train[[['text']]): Creating a dataset using Hugging Face's datasets library.\n\n* def train_corp_iter(): ...: Define a function that creates an iterator by splitting the training data into small chunks.\n\n* raw_tokenizer.train_from_iterator(train_corp_iter(), trainer=trainer): The tokenizer is trained on the training data.\n\n* tokenizer = PreTrainedTokenizerFast(...): Creating a huggingface tokenizer using Hugging Face's PreTrainedTokenizerFast class. This can then be used for model training or tasks such as text attribute extraction.\n\n* tokenized_texts_test and tokenized_texts_train: Using the trained tokenizer, texts from test and training data are tokenized.","metadata":{}},{"cell_type":"code","source":"tokenized_texts_test[1]","metadata":{"papermill":{"duration":0.01862,"end_time":"2024-01-02T03:20:37.487105","exception":false,"start_time":"2024-01-02T03:20:37.468485","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-01-24T21:01:53.237845Z","iopub.execute_input":"2024-01-24T21:01:53.238466Z","iopub.status.idle":"2024-01-24T21:01:53.246531Z","shell.execute_reply.started":"2024-01-24T21:01:53.238432Z","shell.execute_reply":"2024-01-24T21:01:53.245451Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This code will print the second instance in the tokenized_texts_test list.","metadata":{}},{"cell_type":"markdown","source":"<div style=\"color: black;\n            background-color: #800000; /* Doğru bordo tonu */\n            border-radius: 10px; /* Köşeleri yuvarlamak için */\n            padding: 20px; /* Kenar boşluğu eklemek için */\n            box-shadow: 0 0 10px rgba(0, 0, 0, 0.3); /* Hafif gölgelendirme eklemek için */\">\n    <p style=\"color: white; /* Beyaz yazı için */\n              font-family: Verdana, sans-serif;\n              letter-spacing: 0.5px;\n              text-align: center; /* Metni ortalamak için */\n              margin: 0;\">\nKod Blok No:4 / Code Block No.4     </p>\n</div>\n","metadata":{}},{"cell_type":"code","source":"def dummy(text):\n    return text","metadata":{"papermill":{"duration":0.016,"end_time":"2024-01-02T03:20:37.512615","exception":false,"start_time":"2024-01-02T03:20:37.496615","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-01-24T21:01:53.247818Z","iopub.execute_input":"2024-01-24T21:01:53.250685Z","iopub.status.idle":"2024-01-24T21:01:53.262861Z","shell.execute_reply.started":"2024-01-24T21:01:53.250613Z","shell.execute_reply":"2024-01-24T21:01:53.261421Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def dummy(text):\n    return text\n\nvectorizer = TfidfVectorizer(ngram_range=(1, 1), lowercase=False, sublinear_tf=True, analyzer = 'word',\n    tokenizer = dummy,\n    preprocessor = dummy,\n    token_pattern = None, strip_accents='unicode')\n\nvectorizer.fit(tokenized_texts_test)\n\n# Getting vocab\nvocab = vectorizer.vocabulary_\n\nprint(vocab)\n\nvectorizer = TfidfVectorizer(ngram_range=(1, 1), lowercase=False, sublinear_tf=True, vocabulary=vocab,\n                            analyzer = 'word',\n                            tokenizer = dummy,\n                            preprocessor = dummy,\n                            token_pattern = None, strip_accents='unicode'\n                            )\n\ntf_train = vectorizer.fit_transform(tokenized_texts_train)\ntf_test = vectorizer.transform(tokenized_texts_test)\n\ndel vectorizer\ngc.collect()","metadata":{"papermill":{"duration":195.041099,"end_time":"2024-01-02T03:23:52.56314","exception":false,"start_time":"2024-01-02T03:20:37.522041","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-01-24T21:01:53.264448Z","iopub.execute_input":"2024-01-24T21:01:53.265319Z","iopub.status.idle":"2024-01-24T21:02:00.595796Z","shell.execute_reply.started":"2024-01-24T21:01:53.265277Z","shell.execute_reply":"2024-01-24T21:02:00.594648Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This script is designed to convert text data to vectors using TfidfVectorizer. Below is a step-by-step explanation of the code:\n\n* def dummy(text): return text: This is a simple function that directly returns an array of text. The purpose of this function is to provide a tokenization function to TfidfVectorizer.\n\n* vectorizer = TfidfVectorizer(...): Creating a TfidfVectorizer object. This vectorizer is used to convert text data into vectors with the TF-IDF (Term Frequency-Inverse Document Frequency) representation. The expression ngram_range=(1, 1) indicates that it will use unigrams (single words).\n\n* vectorizer.fit(tokenized_texts_test): Fitting the vectorizer on the test data. This step means training the vectorizer and collecting the statistics needed to represent the test data.\n\n* vocab = vectorizer.vocabulary_: The vocabulary_ property is used to get the vocabulary of the vectorizer. This returns a dictionary containing the vocabulary that the vectorizer has learned.\n\n* vectorizer = TfidfVectorizer(..., vocabulary=vocab): This time, the vocabulary obtained in the previous step is specified using the vocabulary parameter. This will be used to convert the training and test data into vectors using the same vocabulary.\n\n* tf_train = vectorizer.fit_transform(tokenized_texts_train): The training data is transformed into TF-IDF representatives using the specified vocabulary.\n\n* tf_test = vectorizer.transform(tokenized_texts_test): Test data is transformed into TF-IDF representatives using the same vocabulary without the need to fit the previous vectorizer.\n\n* del vectorizer and gc.collect(): Deleting the vectorizer from memory and clearing memory. These steps are done to reduce unnecessary memory usage.","metadata":{}},{"cell_type":"markdown","source":"<div style=\"color: black;\n            background-color: #800000; /* Doğru bordo tonu */\n            border-radius: 10px; /* Köşeleri yuvarlamak için */\n            padding: 20px; /* Kenar boşluğu eklemek için */\n            box-shadow: 0 0 10px rgba(0, 0, 0, 0.3); /* Hafif gölgelendirme eklemek için */\">\n    <p style=\"color: white; /* Beyaz yazı için */\n              font-family: Verdana, sans-serif;\n              letter-spacing: 0.5px;\n              text-align: center; /* Metni ortalamak için */\n              margin: 0;\">\nKod Blok No:5 / Code Block No.5     </p>\n</div>\n","metadata":{}},{"cell_type":"code","source":"y_train = train['label'].values","metadata":{"papermill":{"duration":0.016874,"end_time":"2024-01-02T03:23:52.589102","exception":false,"start_time":"2024-01-02T03:23:52.572228","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-01-24T21:02:00.597236Z","iopub.execute_input":"2024-01-24T21:02:00.597667Z","iopub.status.idle":"2024-01-24T21:02:00.603270Z","shell.execute_reply.started":"2024-01-24T21:02:00.597634Z","shell.execute_reply":"2024-01-24T21:02:00.601833Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_model():\n    from catboost import CatBoostClassifier\n\n#     clf2 = MultinomialNB(alpha=0.01)\n    clf = MultinomialNB(alpha=0.0225)\n#     clf2 = MultinomialNB(alpha=0.01)\n    sgd_model = SGDClassifier(max_iter=9000, tol=1e-4, loss=\"modified_huber\", random_state=6743) \n    p6={'n_iter': 3000,'verbose': -1,'objective': 'cross_entropy','metric': 'auc',\n        'learning_rate': 0.00581909898961407, 'colsample_bytree': 0.78,\n        'colsample_bynode': 0.8,\n#         'lambda_l1': 4.562963348932286, \n       # 'lambda_l2': 2.97485, 'min_data_in_leaf': 115, 'max_depth': 23, 'max_bin': 898\n       }\n    p6[\"random_state\"] = 6743\n    lgb=LGBMClassifier(**p6)\n    cat=CatBoostClassifier(iterations=3000,\n                           verbose=0,\n                           random_seed=6543,\n#                            l2_leaf_reg=6.6591278779517808,\n                           learning_rate=0.005599066836106983,\n                           subsample = 0.35,\n                           allow_const_label=True,loss_function = 'CrossEntropy')\n    weights = [0.2,0.31,0.31,0.46]\n \n    ensemble = VotingClassifier(estimators=[('mnb',clf),\n                                            ('sgd', sgd_model),\n                                            ('lgb',lgb), \n                                            ('cat', cat)\n                                           ],\n                                weights=weights, voting='soft', n_jobs=-1)\n    return ensemble\n\nmodel = get_model()\nprint(model)\n\nif len(test.text.values) <= 5:\n    # if not, just sample submission\n    sub.to_csv('submission.csv', index=False)\nelse:\n    model.fit(tf_train, y_train)\n\n    gc.collect()\n\n    final_preds = model.predict_proba(tf_test)[:,1]\n    sub['generated'] = final_preds\n    sub.to_csv('submission.csv', index=False)\n    sub","metadata":{"papermill":{"duration":0.370839,"end_time":"2024-01-02T03:23:52.969039","exception":false,"start_time":"2024-01-02T03:23:52.5982","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-01-24T21:02:00.605019Z","iopub.execute_input":"2024-01-24T21:02:00.605480Z","iopub.status.idle":"2024-01-24T21:02:01.053431Z","shell.execute_reply.started":"2024-01-24T21:02:00.605444Z","shell.execute_reply":"2024-01-24T21:02:01.052413Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This code block creates an ensemble model and uses it to classify or predict the probability of text data. \n\n* get_model() function: This function creates and returns an ensemble model. The ensemble model consists of the MultinomialNB (Naive Bayes), SGDClassifier (Stochastic Gradient Descent), LGBMClassifier (LightGBM), and CatBoostClassifier models. VotingClassifier is used to combine the predictions of these models.\n\n* model = get_model(): The ensemble model is created by calling the get_model function.\n\n* if len(test.text.values) <= 5:: If the number of samples in the test dataset is less than 5, only the sample is used to save a submission file. In this case, the model is not trained and a sample submission file (submission.csv) is created.\n\n* else:: If the number of samples in the test dataset is greater than 5, the model is trained and prediction is performed on the test data. The prediction results are saved in the submission.csv file.\n\n* Note: If you will examine the code in the else block, the line model.fit(tf_train, y_train) performs the training of the model. tf_train refers to a NumPy array containing training data and y_train labels containing TF-IDF representatives. The prediction results are then retrieved using model.predict_proba(tf_test)[:,1] and added to the sub['generated'] column. Finally, these predictions are saved in the submission.csv file.","metadata":{}},{"cell_type":"code","source":"","metadata":{"papermill":{"duration":0.008696,"end_time":"2024-01-02T03:23:52.987086","exception":false,"start_time":"2024-01-02T03:23:52.97839","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"papermill":{"duration":0.008979,"end_time":"2024-01-02T03:23:53.005097","exception":false,"start_time":"2024-01-02T03:23:52.996118","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]}]}